##  Experiment 3: Question Generation Ablation Study Branch

###  Branch Focus

This branch contains all the code and assets necessary to execute **Experiment 3**, which investigates the **Question Generation (QG)** task within our Oral History Dialogue System project.

Our primary goal here is to conduct a **feature ablation study** to quantify the specific impact of various dialogue context features (namely Named Entities and Predicted Emotions) on the quality of questions generated by a Large Language Model (LLM).

The results of this experiment are crucial for determining the optimal input representation for our final dialogue system.

## Structure Overview

| Folder | Purpose and Core Content |
| :--- | :--- |
| **`data_json/`** | **Raw Data (Ground Truth)**: Stores the initial $\mathbf{50}$ high-quality dialogue files generated by an LLM. These files contain the original **`life_stage`**, **`event_type`**, and LLM-generated entity and emotion annotations. |
| **`data_processed/`** | **Intermediate Processed Data**: Contains structured JSON files ($\approx 50$) that have been processed with predicted/merged emotion and NER (entity) labels. These files are ready for use in the downstream generation and experiment modules. |
| **`data_combination/`** | **Data Integration & Cleaning**: Contains scripts (`data_combine.py`, `combine_process.py`) responsible for merging and formatting various annotation sources (e.g., emotion from `emotion/`, entities from `entity/`) to produce the final files required by the `data_processed/` folder. |
| **`dialogue_generation/`** | **Data Generation Script**: Contains the script (`data_generation.py`) used to initialize the entire dataset. It simulates interviews between a professional archivist and $\mathbf{50}$ diverse elderly subjects, generating conversations rich in emotions and events. |
| **`question_generation/`** | **Core Experiment Module (Exp 3)**: Contains all scripts, inputs, and outputs for the **Question Generation Ablation Experiment (ABCD)**. This module investigates the effectiveness of entity and emotion information in prompting LLMs to ask better follow-up questions. |

-----

##  Core Experiment Details (`question_generation`)

This module tests the hypothesis that providing specific, processed annotation features in the dialogue history improves the quality of the next generated interviewer question.

### Experiment Files and Flow

| Script/File | Functionality | Step |
| :--- | :--- | :--- |
| **`data_sampling.py`** | Samples a test set ($\approx 20\%$) and identifies $\mathbf{4}$ specific Subject Turn prediction points per dialogue. Creates `experiment_..._metadata.json`. | **1. Sampling** |
| **`data.py`** | Constructs $\mathbf{4}$ distinct LLM prompts (A, B, C, D) for each sample point based on feature inclusion rules. Outputs `experiment_..._pts_input.json`. | **2. Prompt Construction** |
| **`run_llm_generation.py`** | Calls the OpenAI API (**GPT-4o**) to generate the next question for all $\mathbf{4}$ prompt groups. Outputs `experiment_..._results.json`. | **3. LLM Generation** |
| **`analyze_results.py`** | Calculates ROUGE/BERTScore and uses GPT-4o for **human-like evaluation** (Relevance, Depth, Emotional Engagement, Naturalness). Generates final report tables. | **4. Evaluation Analysis** |

### ABCD Feature Ablation Groups

| Group | Included Annotation Features | Focus/Goal |
| :--- | :--- | :--- |
| **A (Baseline)** | `life_stage`, `event` (Mandatory exclusion of Entity/Emotion) | Relevance based on basic topic flow. |
| **B** | A + **`entities`** (Named Entity Recognition) | Generating **topic-focused** and detail-oriented questions. |
| **C** | A + **`emotions`** (Predicted Emotion Labels) | Generating **emotionally engaging** and empathetic questions. |
| **D (Full)** | A + `entities`, `emotions` | Using **full context** for optimal generation. |

##  Execution Guide

To run the full QG experiment, execute the Python scripts in the `question_generation/` folder sequentially:

1.  **Preparation**: Ensure the necessary input files exist in `../data_processed/` and your OpenAI API Key is configured.
2.  **Run**:
    ```bash
    # 1. Sample Test Set
    python question_generation/data_sampling.py

    # 2. Construct Prompts
    python question_generation/data.py

    # 3. LLM Generation (Requires API Key)
    python question_generation/run_llm_generation.py

    # 4. Evaluate Results
    python question_generation/analyze_results.py
    ```
