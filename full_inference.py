import os
import json
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹è·¯å¾„ (æŒ‡å‘æ‚¨æœ€å¥½çš„æ¨¡å‹)
MODEL_PATH = "./ner_model_roberta_base/final_model" 

# 2. æ•°æ®è·¯å¾„
INPUT_DIR = "./data_json"
OUTPUT_DIR = "./knowledge_base"
# ===========================================

def main():
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„ {MODEL_PATH}")
        return

    if not os.path.exists(INPUT_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶å¤¹ {INPUT_DIR}")
        return

    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH} ...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)
        ner_pipeline = pipeline(
            "token-classification", 
            model=model, 
            tokenizer=tokenizer, 
            aggregation_strategy="simple",
            device=-1 
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.json')]
    print(f"ğŸ“„ å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ (ä¿ç•™å®Œæ•´å¯¹è¯æµ)...")

    for filename in tqdm(files, desc="Processing"):
        input_path = os.path.join(INPUT_DIR, filename)
        output_path = os.path.join(OUTPUT_DIR, f"KB_{filename}")

        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ–°çš„è¾“å‡ºç»“æ„ï¼šåŒ…å«å®Œæ•´çš„å¯¹è¯æµ
            kb_output = {
                "session_id": data.get("session_id", "unknown"),
                "profile": data.get("profile", {}),
                "dialogue_analysis": [] 
            }

            for turn in data.get("dialogue_turns", []):
                speaker = turn.get("speaker")
                turn_id = turn.get("turn_id")
                
                # å®¹å™¨ï¼šç”¨äºå­˜å‚¨è¿™ä¸€è½®çš„åˆ†æç»“æœ
                turn_data = {
                    "turn_id": turn_id,
                    "speaker": speaker,
                    "text_content": "" # ç¨åå¡«å……
                }

                # === æƒ…å†µ A: é‡‡è®¿è€… (åªä¿ç•™æ–‡æœ¬ï¼Œä¸åš NER) ===
                if speaker == "Interviewer":
                    turn_data["text_content"] = turn.get("text", "")
                    # ä¸åŠ  "entities" å­—æ®µï¼Œæˆ–è€…ç•™ç©º
                
                # === æƒ…å†µ B: å—è®¿è€… (ä¿ç•™æ–‡æœ¬ + åš NER) ===
                elif speaker == "Subject":
                    # è·å–å¥å­åˆ—è¡¨ (å…¼å®¹æ–°æ—§æ ¼å¼)
                    sentences = turn.get("sentences", turn.get("sentence_annotations", []))
                    
                    full_text = ""
                    extracted_entities = []

                    for sent in sentences:
                        text = sent.get("text", "")
                        if not text: continue
                        
                        full_text += text + " " # æ‹¼æ¥å®Œæ•´å›ç­”ä»¥ä¾¿é˜…è¯»
                        
                        # --- æ¨¡å‹æ¨ç† ---
                        predictions = ner_pipeline(text)
                        for pred in predictions:
                            extracted_entities.append({
                                "text": pred['word'],
                                "type": pred['entity_group'],
                                "confidence": f"{pred['score']:.4f}"
                            })
                        # ----------------
                    
                    turn_data["text_content"] = full_text.strip()
                    turn_data["extracted_entities"] = extracted_entities

                # å°†å¤„ç†å¥½çš„ä¸€è½®å¯¹è¯åŠ å…¥ç»“æœ
                kb_output["dialogue_analysis"].append(turn_data)

            # ä¿å­˜
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(kb_output, f, indent=2, ensure_ascii=False)

        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {filename}: {e}")

    print(f"âœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜åœ¨: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()