import os
import json
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹è·¯å¾„ (è¯·ç¡®ä¿æŒ‡å‘æ‚¨ F1 åˆ†æ•°æœ€é«˜çš„é‚£ä¸ªæ¨¡å‹)
MODEL_PATH = "./ner_model_roberta_base/final_model" 

# 2. æ•°æ®è·¯å¾„
INPUT_DIR = "./data_json"
OUTPUT_DIR = "./knowledge_base"  # æ¨ç†ç»“æœå°†ä¿å­˜åœ¨è¿™é‡Œ

# ===========================================

def main():
    # 1. æ£€æŸ¥ç¯å¢ƒ
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„ {MODEL_PATH}")
        print("è¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ MODEL_PATH å˜é‡ï¼ŒæŒ‡å‘æ‚¨è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶å¤¹ã€‚")
        return

    if not os.path.exists(INPUT_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶å¤¹ {INPUT_DIR}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“‚ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH} ...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)
        # aggregation_strategy="simple" ä¼šè‡ªåŠ¨åˆå¹¶ B- å’Œ I- æ ‡ç­¾ (ä¾‹å¦‚ "New" + "York" -> "New York")
        ner_pipeline = pipeline(
            "token-classification", 
            model=model, 
            tokenizer=tokenizer, 
            aggregation_strategy="simple",
            device=-1 # å¦‚æœæœ‰GPUæ”¹ç”¨ 0ï¼Œæ²¡æœ‰åˆ™ç”¨ -1 (CPU)
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. è·å–æ–‡ä»¶åˆ—è¡¨
    files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.json')]
    print(f"ğŸ“„ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å…¨é‡æ¨ç†...")

    # 4. å¾ªç¯å¤„ç†æ¯ä¸ªæ–‡ä»¶
    success_count = 0
    
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    for filename in tqdm(files, desc="Processing Files"):
        input_path = os.path.join(INPUT_DIR, filename)
        output_path = os.path.join(OUTPUT_DIR, f"KB_{filename}") # KB = Knowledge Base

        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # å‡†å¤‡å­˜å‚¨æå–ç»“æœçš„ç»“æ„
            extracted_data = {
                "session_id": data.get("session_id", "unknown"),
                "profile": data.get("profile", {}),
                "extracted_knowledge": [] # è¿™é‡Œå­˜æ”¾æ¨¡å‹æå–å‡ºæ¥çš„å®ä½“
            }

            # éå†å¯¹è¯
            for turn in data.get("dialogue_turns", []):
                # æˆ‘ä»¬ä¸»è¦å…³å¿ƒ Subject (å—è®¿è€…) çš„å›ç­”
                if turn.get("speaker") == "Subject" and "sentences" in turn:
                    for sent in turn["sentences"]:
                        text = sent.get("text", "")
                        if not text:
                            continue

                        # === æ ¸å¿ƒæ­¥éª¤ï¼šæ¨¡å‹æ¨ç† ===
                        predictions = ner_pipeline(text)
                        # =======================

                        # æ•´ç†é¢„æµ‹ç»“æœ
                        entities = []
                        for pred in predictions:
                            entities.append({
                                "text": pred['word'],
                                "type": pred['entity_group'],
                                "confidence": f"{pred['score']:.4f}" # ä¿ç•™ç½®ä¿¡åº¦
                            })

                        # åªæœ‰å½“æå–åˆ°å®ä½“æ—¶æ‰ä¿å­˜ï¼Œä¿æŒæ•°æ®æ•´æ´
                        if entities:
                            extracted_data["extracted_knowledge"].append({
                                "turn_id": turn.get("turn_id"),
                                "original_text": text,
                                "predicted_entities": entities
                            })

            # ä¿å­˜ç»“æœæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, indent=2, ensure_ascii=False)
            
            success_count += 1

        except Exception as e:
            print(f"\nâš ï¸ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")

    print("\n" + "="*50)
    print(f"âœ… å…¨é‡æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“Š æˆåŠŸå¤„ç†: {success_count}/{len(files)}")
    print(f"ğŸ“‚ ç»“æœå·²ä¿å­˜åœ¨: {os.path.abspath(OUTPUT_DIR)}")
    print("="*50)

if __name__ == "__main__":
    main()